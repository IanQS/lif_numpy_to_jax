{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fb413687d856ef",
   "metadata": {},
   "source": [
    "# The Leaky Integrate-and-fire\n",
    "\n",
    "<img src=\"./lif_formulation.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "## Notebook Description\n",
    "\n",
    "Here we've restructured the code from the previous notebook, [jax_leaky_integrate_and_fire_1.ipynb](./jax_leaky_integrate_and_fire_1.ipynb), to use the `jit`. Note that we \n",
    "extracted the \"update\" step and moved the spike storage to outside the update function. This is because jax's `jit` requires that the function and contents are immutable. \n",
    "\n",
    "See [exercises/exe_02_jit.ipynb](../../exercises/exe_02_jit.ipynb) for more information\n",
    "\n",
    "## Core Concepts:\n",
    "\n",
    "- `jit`\n",
    "- immutability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e39f555b54ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T01:24:16.107610Z",
     "start_time": "2024-06-15T01:24:15.627392Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from typing import TypeAlias\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from lif_hparams import (\n",
    "    _dt,\n",
    "    _t_max,\n",
    "    _tau_m,\n",
    "    _V_reset,\n",
    "    _V_thresh,\n",
    "    _R,\n",
    "    num_simulations\n",
    ")\n",
    "\n",
    "with open('weights.npy', 'rb') as f:\n",
    "    W = np.load(f)\n",
    "\n",
    "\n",
    "# Initial conditions\n",
    "n_neurons = len(W)# Number of neurons in the network\n",
    "_V = jnp.ones(n_neurons) * _V_reset  # Initial potentials\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c050324757155b",
   "metadata": {},
   "source": [
    "# Type Definitions for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d098bdd4ddd60b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T01:24:16.546421Z",
     "start_time": "2024-06-15T01:24:16.542793Z"
    }
   },
   "outputs": [],
   "source": [
    "Mat: TypeAlias = jnp.ndarray\n",
    "Vec: TypeAlias = jnp.ndarray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb35f0533254f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T01:25:49.434181Z",
     "start_time": "2024-06-15T01:24:16.865949Z"
    }
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_step(\n",
    "        V, W,\n",
    "        # Hyperparameters\n",
    "        v_thresh, v_reset, dt, tau_m, membr_R):\n",
    "    fired = V >= v_thresh\n",
    "    V = jnp.where(fired, v_reset, V)\n",
    "    \n",
    "    # Update voltages\n",
    "    I_syn = W.dot(fired)  # Synaptic current from spikes\n",
    "    dV = (dt / tau_m) * (-V + v_reset + membr_R * I_syn)\n",
    "    V += dV\n",
    "\n",
    "    # No self-inputs; neurons cannot spike themselves in this timestep\n",
    "    V = jnp.where(fired, v_reset, V)\n",
    "    return V, fired\n",
    "\n",
    "\n",
    "def run_simulation(\n",
    "    W: Mat,\n",
    "    V: Vec,\n",
    "\n",
    "    # Neuron Parameters\n",
    "    tau_m: float,\n",
    "    v_reset: float,\n",
    "    v_thresh: float,\n",
    "    membr_R: float,\n",
    "\n",
    "    # How long do we run for? \n",
    "    t_max: float,\n",
    "    dt: float, \n",
    "\n",
    "):\n",
    "    # Simulation\n",
    "\n",
    "    spike_train = []\n",
    "    for i, t in enumerate(jnp.arange(0, t_max, dt)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        V, fired = update_step(V, W, v_thresh, v_reset, dt, tau_m, membr_R )\n",
    "        spike_train.append(fired)\n",
    "\n",
    "    return spike_train\n",
    "\n",
    "time_arr = []\n",
    "for i in range(num_simulations):\n",
    "    start = time.time()\n",
    "    spike_train = run_simulation(\n",
    "        W,\n",
    "        _V,\n",
    "        _tau_m, _V_reset, _V_thresh, _R,\n",
    "        _t_max, _dt\n",
    "    )\n",
    "    np.asarray(spike_train)\n",
    "    end = time.time()\n",
    "    #print(f\"Iteration {i} took: {end - start} seconds\")\n",
    "    time_arr.append(end - start)\n",
    "\n",
    "print(f\"Average Time: {np.mean(time_arr)}\")\n",
    "print(f\"S.Dev Time: {np.std(time_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71232469a2026e3",
   "metadata": {},
   "source": [
    "‼️‼️**Wait** this took more time than the numpy version, what gives? Jax's jit seems doesn't automatically convert our `np.ndarray` to `jnp.ndarray`. So, let's do so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ff0491b9878c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T01:26:10.916286Z",
     "start_time": "2024-06-15T01:25:52.283134Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "time_arr = []\n",
    "for i in range(num_simulations):\n",
    "    start = time.time()\n",
    "    spike_train = run_simulation(\n",
    "        jnp.asarray(W),\n",
    "        _V,\n",
    "        _tau_m, _V_reset, _V_thresh, _R,\n",
    "        _t_max, _dt\n",
    "    )\n",
    "    np.asarray(spike_train)\n",
    "    end = time.time()\n",
    "    #print(f\"Iteration {i} took: {end - start} seconds\")\n",
    "    time_arr.append(end - start)\n",
    "\n",
    "print(f\"Average Time: {np.mean(time_arr)}\")\n",
    "print(f\"S.Dev Time: {np.std(time_arr)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
