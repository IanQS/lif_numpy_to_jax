{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5eb3a90919eb4a",
   "metadata": {},
   "source": [
    "# Gaussian_Mixture_Model\n",
    "\n",
    "Concepts used:\n",
    "\n",
    "- `jit`: make our code move blazingly fast. See [exe_02_jit](../../exercises/exe_02_jit.ipynb) and [Jax jit docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html)\n",
    "- `vmap`: automatically vectorize our `e_step` and `m_step` across the matrices of data. See [exe_04_vmap](../../exercises/exe_04_vmap.ipynb) and [jax vmap docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html)\n",
    "- composing `jit` and `vmap`. The functional programming nature of `jax` means that we can compose these higher-order functions, making our code look very clean. See the `e_step` for more. \n",
    "\n",
    "**Additional Concepts**\n",
    "We also introduce the `RNG` and `PMAP` concepts at the very bottom, where we start our GMM from multiple random locations. For more information on the \n",
    "\n",
    "Before each newly introduced concept we briefly discuss the arguments and why the code is laid out the way it is :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93ff9ca2164e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:40:54.238198Z",
     "start_time": "2024-06-21T14:40:53.252723Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.scipy.stats.norm import pdf as n_pdf\n",
    "import numpy as np  \n",
    "np.random.seed(123)\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587dfd8389a75a9",
   "metadata": {},
   "source": [
    "# Reporting Functions\n",
    "\n",
    "Here we do not use a `vmap` and instead only use a `jit`. Note how we do: `to_exp = -0.5 * jnp.sum(diff @ inv * diff, axis=1)`, which looks nothing like the formula we see on Wikipedia\n",
    "\n",
    "![](./gaussian_pdf.png)\n",
    "\n",
    "Later on in the code we show an alternative form, where we use a `vmap` to avoid this strange looking code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94728da61ccc26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:40:55.029359Z",
     "start_time": "2024-06-21T14:40:55.025181Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use a jit to speed up the function. a `jit` + `vmap` should achieve the same performance\n",
    "@jax.jit\n",
    "def gaussian_pdf(coor: jnp.array, mu_k: jnp.array, sigma_k: jnp.array) -> jnp.array:\n",
    "    # PDF formula from: https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "    k = len(mu_k)\n",
    "    t1 = (2 * jnp.pi) ** (-k / 2)\n",
    "    t2 = jnp.linalg.det(sigma_k) ** (-0.5)\n",
    "\n",
    "    inv = jnp.linalg.inv(sigma_k)\n",
    "    diff = coor - mu_k\n",
    "    to_exp = -0.5 * jnp.sum(diff @ inv * diff, axis=1)\n",
    "\n",
    "    to_ret = t1 * t2 * jnp.exp(to_exp)\n",
    "\n",
    "    assert len(to_ret) == len(coor)\n",
    "    return to_ret\n",
    "\n",
    "def log_likelihood(data, mu, sigma, pi, K):\n",
    "    log_likelihood = 0\n",
    "    for data_point in data:\n",
    "        mixture_likelihood = 0\n",
    "        for k in range(K):\n",
    "            v = pi[k] * gaussian_pdf(\n",
    "                jnp.expand_dims(data_point, axis=0), mu_k=mu[k], sigma_k=sigma[k]\n",
    "            )\n",
    "            mixture_likelihood += v\n",
    "        log_likelihood += np.log(mixture_likelihood)\n",
    "\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9e970eaccbe68",
   "metadata": {},
   "source": [
    "# 1) Helper Functions for `e_step` \n",
    "\n",
    "- `_loglikelihood_gaussian` this is a log-likelihood calculation that operates on a single row of x, hence the $x_i$ notation. Because we have formulated it this way, we see that our code from before (after applying the log) has become `0.5 * diff @ sigma_inv @ diff`\n",
    "\n",
    "- `ll_gaussian` this applies the `_loglikelihood_gaussian` across elements of the first argument. Notice how we have `in_axes=(0, None, None, None)`, which means we vectorize over the 0-th axis of the first argument, which is the data matrix in our example.\n",
    "\n",
    "- `calculate_normalizer`: no special tricks here, just calculating the normalizer for the [log-sum-exp](https://en.wikipedia.org/wiki/LogSumExp) trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb67f87ce24853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:40:56.078361Z",
     "start_time": "2024-06-21T14:40:56.074272Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def _loglikelihood_gaussian(x_i: jnp.array, cls_prior_k: jnp.array, mu_k: jnp.array, sigma_k: jnp.array) -> jnp.array:\n",
    "    \"\"\"\n",
    "    Calculate the LL for a single point\n",
    "    Args:\n",
    "        x_i: vector of shape (1, num_feats)\n",
    "        mu_k: vector of shape (1, num_feats)\n",
    "        sigma_k: matrix of shape (num_feats, num_feats)\n",
    "\n",
    "    \"\"\"\n",
    "    k = len(mu_k)\n",
    "    sigma_inv = jnp.linalg.inv(sigma_k)\n",
    "    sigma_det = jnp.linalg.det(sigma_k)\n",
    "    log_det_sigma = jnp.log(sigma_det)\n",
    "\n",
    "    diff = x_i - mu_k\n",
    "    t1 = -0.5 * k * jnp.log(2 * jnp.pi)\n",
    "    t2 = -0.5 * log_det_sigma\n",
    "    t3 = -0.5 * diff @ sigma_inv @ diff\n",
    "\n",
    "    return t1 + t2 + t3 + jnp.log(cls_prior_k)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def ll_gaussian(X, cls_prior_k, mu_k, sigma_k):\n",
    "    return jax.vmap(\n",
    "        fun=_loglikelihood_gaussian,\n",
    "        in_axes=(0, None, None, None)\n",
    "    )(X, cls_prior_k, mu_k, sigma_k)\n",
    "\n",
    "@jax.jit\n",
    "def calculate_normalizer(log_prob_arr: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    For the log-sum-exp\n",
    "    Args:\n",
    "        log_prob_arr:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    _max = jnp.max(log_prob_arr, axis=0)\n",
    "    return _max + jnp.log(jnp.sum(\n",
    "        jnp.exp(log_prob_arr - _max),\n",
    "        axis=0\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb537ae36a5678c3",
   "metadata": {},
   "source": [
    "# 2) EM Algorithm\n",
    "\n",
    "Reference: [Wiki: EM_algorithm_and_GMM_model](https://en.wikipedia.org/wiki/EM_algorithm_and_GMM_model)\n",
    "\n",
    "In the **E-step** we calculate the weights for the updates based on how likely a point is to have come from one of the specified clusters\n",
    "\n",
    "In the **M-step** we calculate the update for the parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b85e63d07960b",
   "metadata": {},
   "source": [
    "## `e_step`\n",
    "\n",
    "we use the `ll_gaussian` which maps `_loglikelihood_gaussian` over each row of the dataset. We have actually `vmap`-ed the `ll_gaussian` \n",
    "over each of the parameters of the `mus`, `sigmas` and `cls_priors`, all of which are of shape (4, ...), which means that we are \"indexing\" over all of them at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe76426a31a6372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:40:57.358429Z",
     "start_time": "2024-06-21T14:40:57.355605Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def e_step(X, mus, sigmas, cls_priors):\n",
    "    ll_gaussian_over_parameters = jax.vmap(\n",
    "        ll_gaussian,\n",
    "        in_axes=(None, 0, 0, 0)\n",
    "    )\n",
    "    _responsibilities = ll_gaussian_over_parameters(X, cls_priors, mus, sigmas)\n",
    "    normalizer = calculate_normalizer(_responsibilities)\n",
    "    return jnp.exp(_responsibilities - normalizer).T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931bd4e084ac217",
   "metadata": {},
   "source": [
    "## m-step\n",
    "\n",
    "`_m_step_single`: we calculate the mu and sigma for a single row, which we will sum up later\n",
    "\n",
    "`_m_step`: given a specific class, k, get the corresponding `responsibility` and `mu`, then calculate the updated `mu`, `sigma`, and class prior. Our `X` if of shape (N, M) and our `resp_k` is of shape (N, 1), so we map over the 0-th axis\n",
    "\n",
    "`m_step`: we map our `_m_step`, that is conditioned on the class parameters, across the different class parameters. The `responsibilities` are of shape (N, K) and our `mus` are of shape (K, D), so when we do the `vmap`, we map \n",
    "over the 0-th axis of `mu` and the 1st axis of `responsibilities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b1de057a1ef4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:40:58.292200Z",
     "start_time": "2024-06-21T14:40:58.285887Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def _m_step_single(x_i, mu_k, resp_nk):\n",
    "    \"\"\"\n",
    "    Calculate the individual values for mu and sigma\n",
    "    \"\"\"\n",
    "\n",
    "    mu_new = x_i * resp_nk\n",
    "    diff = x_i - mu_k\n",
    "    sigma_new = resp_nk * jnp.outer(diff, diff)\n",
    "    return mu_new, sigma_new\n",
    "\n",
    "@jax.jit\n",
    "def _m_step(X, mu_k, resp_k):\n",
    "    N_k = jnp.sum(resp_k)\n",
    "    to_ave_mus, to_ave_sigmas = jax.vmap(\n",
    "        _m_step_single,\n",
    "        in_axes=(0, None, 0)\n",
    "    )(X, mu_k, resp_k)\n",
    "\n",
    "    mus = jnp.sum(to_ave_mus, axis=0) / N_k\n",
    "    sigmas = jnp.sum(to_ave_sigmas, axis=0) / N_k\n",
    "    cls_prior = N_k / len(X)\n",
    "    return mus, sigmas, cls_prior\n",
    "\n",
    "@jax.jit\n",
    "def m_step(X, mus, responsibilities):\n",
    "    mus, sigmas, cls_prior = jax.vmap(\n",
    "        _m_step,\n",
    "        in_axes=(None, 0, 1)\n",
    "    )(X, mus, responsibilities)\n",
    "\n",
    "    return mus, sigmas, jnp.expand_dims(cls_prior, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8f0cabb8427d6",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8dfe2d2330005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:41:42.297349Z",
     "start_time": "2024-06-21T14:41:42.292927Z"
    }
   },
   "outputs": [],
   "source": [
    "def EM_GMM(\n",
    "        data: np.ndarray,\n",
    "        guess_num_classes,\n",
    "        # Initial guesses\n",
    "        mus, sigmas, cls_probs,\n",
    "        \n",
    "        verbose=False\n",
    "        \n",
    "):\n",
    "    counter = 0\n",
    "    ll_container = []\n",
    "    TOL = 0.00001\n",
    "    ll_container.append(np.inf)\n",
    "\n",
    "    while True:  # Run until converges\n",
    "        # e-step\n",
    "        responsibilities = e_step(data, mus, sigmas, cls_probs)\n",
    "\n",
    "        # m-step\n",
    "        mus, sigmas, cls_probs = m_step(data, mus, responsibilities)\n",
    "        # Recalculate the log-likelihood\n",
    "        ll_curr = float(log_likelihood(data, mus, sigmas, cls_probs, guess_num_classes))\n",
    "\n",
    "        if np.abs(ll_container[-1] - ll_curr) < TOL:\n",
    "            print(f\"Converged to within {TOL} after: {counter} iterations\")\n",
    "            break\n",
    "\n",
    "        ll_container.append(float(ll_curr))\n",
    "        if verbose:\n",
    "            print(f\"Data Log-Likelihood at iteration: {counter} = {ll_curr:.6f}\")\n",
    "        counter += 1\n",
    "\n",
    "    responsibilities = e_step(data, mus, sigmas, cls_probs)\n",
    "    return mus, sigmas, cls_probs.T, responsibilities.T, ll_container[1:]\n",
    "    # -------------------------- #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2650207ec992b3",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "Our problem formulation is where there are 4 actual clusters (we do not know this). In a real-world scenario, we do not know the number of clusters ahead-of-time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d612243a82df7f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:41:43.438711Z",
     "start_time": "2024-06-21T14:41:43.434559Z"
    }
   },
   "outputs": [],
   "source": [
    "unknown_centers = np.asarray([\n",
    "    [1, -1],  # bottom left\n",
    "    [5, 5],  # middle\n",
    "    [8, 7],  # mid-right\n",
    "    [10, 0]  # bottom right\n",
    "])\n",
    "\n",
    "def make_ds(centers):\n",
    "    points_in_classes = [30, 50, 20, 5]\n",
    "    ################################################\n",
    "    # Initial Guesses\n",
    "    ################################################\n",
    "    # Randomly increase/ decrease by 10% each way\n",
    "    scale = (np.random.randint(low=0, high=20, size=centers.shape) - 10) / 100\n",
    "\n",
    "    initial_mu_guesses = centers + (centers * scale)\n",
    "    return make_blobs(points_in_classes, centers=centers), initial_mu_guesses\n",
    "\n",
    "(X, y), mus = make_ds(unknown_centers)\n",
    "\n",
    "N, M = X.shape\n",
    "K = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ac35cd1558c60",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148bd2472eca73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:41:59.443147Z",
     "start_time": "2024-06-21T14:41:55.127882Z"
    }
   },
   "outputs": [],
   "source": [
    "sigmas = np.asarray([np.cov(X.T) for _ in range(K)])\n",
    "cls_probs = np.asarray([1 / K for _ in range(K)]).T\n",
    "cls_probs = np.expand_dims(cls_probs, axis=-1)\n",
    "mus, sigmas, cls_priors, _, lls = EM_GMM(\n",
    "    X,\n",
    "    mus=mus,\n",
    "    sigmas=sigmas,\n",
    "    cls_probs=cls_probs,\n",
    "    guess_num_classes=K,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e90ab22ecd2da",
   "metadata": {},
   "source": [
    "# Result Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b828210f2680835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:42:02.198324Z",
     "start_time": "2024-06-21T14:42:01.934446Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c282a6ef7d6948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:42:02.491690Z",
     "start_time": "2024-06-21T14:42:02.390922Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a972edcead218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:42:20.782290Z",
     "start_time": "2024-06-21T14:42:20.778781Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "def confidence_ellipse(mu, sigma, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Modified based on function from: https://matplotlib.org/stable/gallery/statistics/confidence_ellipse.html\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The Axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    pearson = sigma[0, 1]/np.sqrt(sigma[0, 0] * sigma[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(sigma[0, 0]) * n_std\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(sigma[1, 1]) * n_std\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mu[0], mu[1])\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd816d30afbdaa83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:42:33.814110Z",
     "start_time": "2024-06-21T14:42:33.442567Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "colors = [\"r\", \"g\", \"b\", \"y\"]\n",
    "\n",
    "for i, c in enumerate(colors):\n",
    "    \n",
    "    # Plot the centers\n",
    "    plt.scatter(unknown_centers[i, 0], unknown_centers[i, 1], c=c, marker=\"o\", label=f\"Cluster: {i} True Center\")\n",
    "    plt.scatter(mus[i, 0], mus[i, 1], c=c, marker=\"^\", label=f\"Cluster: {i} Inferred Center\")\n",
    "    \n",
    "    print(f\"True vs Inferred: {unknown_centers[i]} and {mus[i]}\")\n",
    "\n",
    "    # Plot the standard deviations\n",
    "    mask = y == i\n",
    "    masked_points = X[mask]\n",
    "    mu_x = np.mean(masked_points, axis=0)\n",
    "    sigma = np.cov(masked_points[:, 0], masked_points[:, 1])\n",
    "    confidence_ellipse(mu_x, sigma,  ax=axs, n_std=1, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=2, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=3, edgecolor=c, linestyle=\"-\")\n",
    "\n",
    "\n",
    "    confidence_ellipse(mus[i], sigmas[i],  ax=axs, n_std=1, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=2, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=3, edgecolor=c, linestyle=\"--\")\n",
    "plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a1a769abd426d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T14:42:21.721189Z",
     "start_time": "2024-06-21T14:42:21.719350Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73bb70dfe8ac144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
