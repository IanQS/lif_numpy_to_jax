{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5eb3a90919eb4a",
   "metadata": {},
   "source": [
    "# Core Gaussian Mixture Model\n",
    "\n",
    "Here we implement a Gaussian Mixture Model to illustrate how to use Jax. In a followup tutorial, we will cover how to use further concepts like `rng` to ensure reproducible runs, and `pmap` for parallelism across devices.\n",
    "\n",
    "We make use of the following concepts:\n",
    "\n",
    "- `jit`: to accelerate our code. See [exe_02_jit](../../exercises/exe_02_jit.ipynb) and [Jax jit docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) for more information\n",
    "- `vmap`: to automatically vectorize our `e_step` and `m_step` across the matrices of data. See [exe_04_vmap](../../exercises/exe_04_vmap.ipynb) and [jax vmap docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html) for more information\n",
    "- composing `jit` and `vmap`. The functional programming nature of `jax` means that we can compose these higher-order functions, making our code look very clean. See the `e_step` to see this in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06eb9c8bb87557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:52.590195Z",
     "start_time": "2024-06-24T21:48:52.589094Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93ff9ca2164e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:53.027766Z",
     "start_time": "2024-06-24T21:48:52.590612Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.scipy.stats.norm import pdf as n_pdf\n",
    "import numpy as np  \n",
    "np.random.seed(123)\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a3b046e1fad82",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "Say we are trying to model a simple problem where we have 105 points that are 2-d. We know that there are roughly 4 clusters* and we know more-or-less where they start. However, we want to learn the parameters to fit them.\n",
    "\n",
    "*In the next notebook we do not assume that we have this prior knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9193515d061a1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:53.031666Z",
     "start_time": "2024-06-24T21:48:53.028337Z"
    }
   },
   "outputs": [],
   "source": [
    "unknown_centers = np.asarray([\n",
    "    [1, -1],  # bottom left\n",
    "    [5, 5],  # middle\n",
    "    [8, 7],  # mid-right\n",
    "    [10, 0]  # bottom right\n",
    "])\n",
    "\n",
    "def make_ds(centers):\n",
    "    points_in_classes = [30, 50, 20, 5]\n",
    "    ################################################\n",
    "    # Initial Guesses\n",
    "    ################################################\n",
    "    # Randomly increase/ decrease by 15% each way\n",
    "    scale = (np.random.randint(low=0, high=30, size=centers.shape) - 15) / 100\n",
    "\n",
    "    initial_mu_guesses = centers + (centers * scale)\n",
    "    return make_blobs(points_in_classes, centers=centers), initial_mu_guesses\n",
    "\n",
    "(X, y), mus = make_ds(unknown_centers)\n",
    "\n",
    "N, M = X.shape\n",
    "K = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587dfd8389a75a9",
   "metadata": {},
   "source": [
    "# Reporting Functions\n",
    "\n",
    "The following is an implementation of the [Multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution), which we use to enumerate how well our parameters have fit the data. Other implementations of `gaussian_pdf` might\n",
    "include a `einsum`, or a `vmap`, but we only use the `jit`. \n",
    "\n",
    "![](assets/gaussian_pdf.png)\n",
    "\n",
    "Note how the expression in the `exp(...)` has been implemented as `-0.5 * jnp.sum(diff @ inv * diff, axis=1)`, which looks vastly different from the formula. Further below, in the `_loglikelihood_gaussian` function we see how we can avoid this by mapping\n",
    "our function over the entire array (without sacrificing speed!) If this isn't clear to you, please read through [exe_02_jit](../../exercises/exe_02_jit.ipynb) and [exe_05_profiling](../../exercises/exe_05_profiling.ipynb) to see this in action, and to see the timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94728da61ccc26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:53.043264Z",
     "start_time": "2024-06-24T21:48:53.032209Z"
    }
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def gaussian_pdf(coor: jnp.array, mu_k: jnp.array, sigma_k: jnp.array) -> jnp.array:\n",
    "    k = len(mu_k)\n",
    "    t1 = (2 * jnp.pi) ** (-k / 2)\n",
    "    t2 = jnp.linalg.det(sigma_k) ** (-0.5)\n",
    "\n",
    "    inv = jnp.linalg.inv(sigma_k)\n",
    "    diff = coor - mu_k\n",
    "    to_exp = -0.5 * jnp.sum(diff @ inv * diff, axis=1)\n",
    "\n",
    "    to_ret = t1 * t2 * jnp.exp(to_exp)\n",
    "\n",
    "    assert len(to_ret) == len(coor)\n",
    "    return to_ret\n",
    "\n",
    "def log_likelihood(data, mu, sigma, pi, K):\n",
    "    log_likelihood = 0\n",
    "    for data_point in data:\n",
    "        mixture_likelihood = 0\n",
    "        for k in range(K):\n",
    "            v = pi[k] * gaussian_pdf(\n",
    "                jnp.expand_dims(data_point, axis=0), mu_k=mu[k], sigma_k=sigma[k]\n",
    "            )\n",
    "            mixture_likelihood += v\n",
    "        log_likelihood += np.log(mixture_likelihood)\n",
    "\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9e970eaccbe68",
   "metadata": {},
   "source": [
    "# 1) Expectation step: `e_step` \n",
    "\n",
    "![](./assets/e_step.png)\n",
    "\n",
    "**Note** the image above was taken (with permission) from [Prof. Matt Golub](https://homes.cs.washington.edu/~mgolub/)'s course, [Machine Learning for Neuroscience (CSE599N)](https://courses.cs.washington.edu/courses/cse599n/24sp/).\n",
    "\n",
    "## The E-step comprises two sub-functions:\n",
    "\n",
    "1) the log-likelihood of the data over the parameters, \n",
    "2) the \"normalizer\" for the [log-sum-exp](https://en.wikipedia.org/wiki/LogSumExp) trick. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570555b7820c6223",
   "metadata": {},
   "source": [
    "### Log-likelihood Gaussian\n",
    "\n",
    "This is a log-likelihood calculation that operates on a single row of x, hence the $x_i$ notation. Because we have formulated it this way, we see that our code from before (after applying the log) has become `0.5 * diff @ sigma_inv @ diff`. What's nice\n",
    "about this is that it makes the equation look closer to the underlying math than whatever tricks we had to pull off earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46b9d55ac55bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:53.049785Z",
     "start_time": "2024-06-24T21:48:53.043701Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def _loglikelihood_gaussian(x_i: jnp.array, cls_prior_k: jnp.array, mu_k: jnp.array, sigma_k: jnp.array) -> jnp.array:\n",
    "    \"\"\"\n",
    "    Calculate the LL for a single point\n",
    "    Args:\n",
    "        x_i: vector of shape (1, num_feats)\n",
    "        mu_k: vector of shape (1, num_feats)\n",
    "        sigma_k: matrix of shape (num_feats, num_feats)\n",
    "\n",
    "    \"\"\"\n",
    "    k = len(mu_k)\n",
    "    sigma_inv = jnp.linalg.inv(sigma_k)\n",
    "    sigma_det = jnp.linalg.det(sigma_k)\n",
    "    log_det_sigma = jnp.log(sigma_det)\n",
    "\n",
    "    diff = x_i - mu_k\n",
    "    t1 = -0.5 * k * jnp.log(2 * jnp.pi)\n",
    "    t2 = -0.5 * log_det_sigma\n",
    "    t3 = -0.5 * diff @ sigma_inv @ diff\n",
    "\n",
    "    return t1 + t2 + t3 + jnp.log(cls_prior_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a83e546d3d1fdb",
   "metadata": {},
   "source": [
    "## e_step\n",
    "\n",
    "The `e_step` function composes two `vmap` applications, one after the other. The first one, \n",
    "\n",
    "```python\n",
    "_ll_gaussian_over_rows_given_parameters = jax.vmap(\n",
    "    fun=_loglikelihood_gaussian,\n",
    "    in_axes=(0, None, None, None)\n",
    ")\n",
    "```\n",
    "\n",
    "applies the `_loglikelihood_gaussian` function over the 0th axis of the first argument, in this case, our `X` data. As the name suggests, this function assumes that we are already \"given\" the parameters. Let's do that now.\n",
    "\n",
    "```python\n",
    "ll_gaussian_over_parameters = jax.vmap(\n",
    "    _ll_gaussian_over_rows_given_parameters,\n",
    "    in_axes=(None, 0, 0, 0)\n",
    ")\n",
    "```\n",
    "\n",
    "Here we map the `_ll_gaussian_over_rows_given_parameters` over the 0th axis of the last 3 arguments **simultaneously**, which means that the 0th axis of those arguments must have the same length.\n",
    "\n",
    "- `calculate_normalizer`: no special tricks here, just calculating the normalizer for the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090d8f6cc4dfefa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:53.057258Z",
     "start_time": "2024-06-24T21:48:53.050142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def e_step(X, mus, sigmas, cls_priors):\n",
    "    \"\"\"\n",
    "    X: (N, M)\n",
    "    mus: (K, M)\n",
    "    sigmas: (K, M, M)\n",
    "    cls_priors: (K, M)\n",
    "    \"\"\"\n",
    "    _ll_gaussian_over_rows_given_parameters = jax.vmap(\n",
    "        fun=_loglikelihood_gaussian,\n",
    "        in_axes=(0, None, None, None)\n",
    "    )\n",
    "    ll_gaussian_over_parameters = jax.vmap(\n",
    "        _ll_gaussian_over_rows_given_parameters,\n",
    "        in_axes=(None, 0, 0, 0)\n",
    "    )\n",
    "    _responsibilities = ll_gaussian_over_parameters(X, cls_priors, mus, sigmas)\n",
    "    normalizer = calculate_normalizer(_responsibilities)\n",
    "    return jnp.exp(_responsibilities - normalizer).T[-1]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def calculate_normalizer(log_prob_arr: jnp.ndarray) -> jnp.ndarray:\n",
    "    _max = jnp.max(log_prob_arr, axis=0)\n",
    "    return _max + jnp.log(jnp.sum(\n",
    "        jnp.exp(log_prob_arr - _max),\n",
    "        axis=0\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931bd4e084ac217",
   "metadata": {},
   "source": [
    "# 2) Maximization-step `m_step`\n",
    "\n",
    "![](./assets/m_step.png)\n",
    "\n",
    "**Note** the image above was taken (with permission) from [Prof. Matt Golub](https://homes.cs.washington.edu/~mgolub/)'s course, [Machine Learning for Neuroscience (CSE599N)](https://courses.cs.washington.edu/courses/cse599n/24sp/).\n",
    "\n",
    "## The M-step comprises two sub-functions:\n",
    "\n",
    "1) calculating the parameter update (within the for-loop)\n",
    "2) calculating the mean\n",
    "3) the top-level caller\n",
    "\n",
    "___\n",
    "\n",
    "In `_m_step_single`, we calculate the mu and sigma for a single row, which we will sum up later. We only need to do this for the `mu` and `sigma` update\n",
    "\n",
    "In `_m_step` we are given the class specific parameters for class k (`responsibility` and `mu`). We use these parameters to calculate the updated `mu`, `sigma`, and class prior. Our `X` if of shape (N, M) and our `resp_k` is of shape (N, 1), so we map over the 0-th axis of both of them at the same time.\n",
    "\n",
    "`m_step`: we map our `_m_step`, that is conditioned on the class parameters, across the different class parameters. The `responsibilities` are of shape (N, K) and our `mus` are of shape (K, D), so when we do the `vmap`, we map \n",
    "over the 0-th axis of `mu` and the 1st axis of `responsibilities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b1de057a1ef4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:53.063677Z",
     "start_time": "2024-06-24T21:48:53.057607Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def _m_step_single(x_i, mu_k, resp_nk):\n",
    "    \"\"\"\n",
    "    Calculate the individual values for mu and sigma\n",
    "    \"\"\"\n",
    "\n",
    "    mu_new = x_i * resp_nk\n",
    "    diff = x_i - mu_k\n",
    "    sigma_new = resp_nk * jnp.outer(diff, diff)\n",
    "    return mu_new, sigma_new\n",
    "\n",
    "@jax.jit\n",
    "def _m_step(X, mu_k, resp_k):\n",
    "    N_k = jnp.sum(resp_k)\n",
    "    to_ave_mus, to_ave_sigmas = jax.vmap(\n",
    "        _m_step_single,\n",
    "        in_axes=(0, None, 0)\n",
    "    )(X, mu_k, resp_k)\n",
    "\n",
    "    mus = jnp.sum(to_ave_mus, axis=0) / N_k\n",
    "    sigmas = jnp.sum(to_ave_sigmas, axis=0) / N_k\n",
    "    cls_prior = N_k / len(X)\n",
    "    return mus, sigmas, cls_prior\n",
    "\n",
    "@jax.jit\n",
    "def m_step(X, mus, responsibilities):\n",
    "    mus, sigmas, cls_prior = jax.vmap(\n",
    "        _m_step,\n",
    "        in_axes=(None, 0, 1)\n",
    "    )(X, mus, responsibilities)\n",
    "\n",
    "    return mus, sigmas, jnp.expand_dims(cls_prior, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8f0cabb8427d6",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8dfe2d2330005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:53.068043Z",
     "start_time": "2024-06-24T21:48:53.063979Z"
    }
   },
   "outputs": [],
   "source": [
    "def EM_GMM(\n",
    "        data: np.ndarray,\n",
    "        guess_num_classes,\n",
    "        # Initial guesses\n",
    "        mus, sigmas, cls_probs,\n",
    "        \n",
    "        verbose=False\n",
    "        \n",
    "):\n",
    "    counter = 0\n",
    "    ll_container = []\n",
    "    TOL = 0.00001\n",
    "    ll_container.append(np.inf)\n",
    "\n",
    "    while True:  # Run until converges\n",
    "        # e-step\n",
    "        responsibilities = e_step(data, mus, sigmas, cls_probs)\n",
    "\n",
    "        # m-step\n",
    "        mus, sigmas, cls_probs = m_step(data, mus, responsibilities)\n",
    "        # Recalculate the log-likelihood\n",
    "        ll_curr = float(log_likelihood(data, mus, sigmas, cls_probs, guess_num_classes))\n",
    "\n",
    "        if np.abs(ll_container[-1] - ll_curr) < TOL:\n",
    "            print(f\"Converged to within {TOL} after: {counter} iterations\")\n",
    "            break\n",
    "\n",
    "        ll_container.append(float(ll_curr))\n",
    "        if verbose:\n",
    "            print(f\"Data Log-Likelihood at iteration: {counter} = {ll_curr:.6f}\")\n",
    "        counter += 1\n",
    "\n",
    "    responsibilities = e_step(data, mus, sigmas, cls_probs)\n",
    "    return mus, sigmas, cls_probs.T, responsibilities.T, ll_container[1:]\n",
    "    # -------------------------- #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ac35cd1558c60",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148bd2472eca73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:57.437847Z",
     "start_time": "2024-06-24T21:48:53.068645Z"
    }
   },
   "outputs": [],
   "source": [
    "# We simply say the covariance for each cluster is the covariance of the entire structure\n",
    "sigmas = np.asarray([np.cov(X.T) for _ in range(K)])\n",
    "\n",
    "# We simply \n",
    "cls_probs = np.expand_dims(\n",
    "    np.asarray([1 / K for _ in range(K)]).T,\n",
    "    axis=-1\n",
    ")\n",
    "mus, sigmas, cls_priors, _, lls = EM_GMM(\n",
    "    X,\n",
    "    mus=mus,\n",
    "    sigmas=sigmas,\n",
    "    cls_probs=cls_probs,\n",
    "    guess_num_classes=K,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e90ab22ecd2da",
   "metadata": {},
   "source": [
    "# Result Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b828210f2680835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:57.596472Z",
     "start_time": "2024-06-24T21:48:57.438268Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f6b4b0716139",
   "metadata": {},
   "source": [
    "## Plot the Log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c282a6ef7d6948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:57.645027Z",
     "start_time": "2024-06-24T21:48:57.596963Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(lls)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log-likelihood of points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f8adff3d7d5ce",
   "metadata": {},
   "source": [
    "# Show the Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a972edcead218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:57.647486Z",
     "start_time": "2024-06-24T21:48:57.645409Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "def confidence_ellipse(mu, sigma, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Modified based on function from: https://matplotlib.org/stable/gallery/statistics/confidence_ellipse.html\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The Axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    pearson = sigma[0, 1]/np.sqrt(sigma[0, 0] * sigma[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(sigma[0, 0]) * n_std\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(sigma[1, 1]) * n_std\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mu[0], mu[1])\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd816d30afbdaa83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:48:57.975695Z",
     "start_time": "2024-06-24T21:48:57.647771Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "colors = [\"r\", \"g\", \"b\", \"y\"]\n",
    "\n",
    "for i, c in enumerate(colors):\n",
    "    \n",
    "    # Plot the centers\n",
    "    plt.scatter(unknown_centers[i, 0], unknown_centers[i, 1], c=c, marker=\"o\", label=f\"Cluster: {i} True Center\")\n",
    "    plt.scatter(mus[i, 0], mus[i, 1], c=c, marker=\"^\", label=f\"Cluster: {i} Inferred Center\")\n",
    "    \n",
    "    # Plot the standard deviations\n",
    "    mask = y == i\n",
    "    masked_points = X[mask]\n",
    "    mu_x = np.mean(masked_points, axis=0)\n",
    "    sigma = np.cov(masked_points[:, 0], masked_points[:, 1])\n",
    "    confidence_ellipse(mu_x, sigma,  ax=axs, n_std=1, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=2, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=3, edgecolor=c, linestyle=\"-\")\n",
    "\n",
    "\n",
    "    confidence_ellipse(mus[i], sigmas[i],  ax=axs, n_std=1, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=2, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=3, edgecolor=c, linestyle=\"--\")\n",
    "plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8d95b632922fc",
   "metadata": {},
   "source": [
    "# Followup\n",
    "\n",
    "Check out [advanced Jax](./additional_jax_gaussian_mixture_model.ipynb) to see Jax's `rng` and `pmap` in action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
