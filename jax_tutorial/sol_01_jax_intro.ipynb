{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8623bd1c-abab-4eaf-bd15-4f78d335de0b",
   "metadata": {},
   "source": [
    "# Intro to Jax a.k.a swapping `np.X` for `jnp.X`\n",
    "\n",
    "## Lesson Goals:\n",
    "\n",
    "By the end of this lesson, you will have an understanding of how to migrate from `numpy` to `jax`, and get a feel for how similar the two libraries can be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba69c5-8ea0-4d65-a69d-f888d06d2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import TypeAlias\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f80b94-03ee-46a6-86b5-5e773b4f1095",
   "metadata": {},
   "source": [
    "# What is Jax?\n",
    "\n",
    "To put it simply, Jax functions as a version of numpy for various hardware accelerators. However, it offers much more than that by providing higher-level abstractions, utilizing a different backend (XLA), and supporting automatic differentiation.\n",
    "\n",
    "From the website:\n",
    "\n",
    "> JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
    "\n",
    "Despite these capabilities, not all concepts and idioms from NumPy translate directly, and there are certain ‼️sharp edges‼️ of which you should be aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256009c1-261b-4f65-a78c-177f9997e390",
   "metadata": {},
   "source": [
    "## Sample Exercises\n",
    "\n",
    "Below, we provide some exercises to help you become familiar with Jax and Numpy. The solutions are more or less what you might expect from a drop-in replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e2125-7b08-4425-b542-59481cd9df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dot_product():\n",
    "    v = jnp.asarray(np.random.rand(10))\n",
    "    M = jnp.asarray(np.random.rand(10, 5))\n",
    "    ########################################\n",
    "    # Your code here\n",
    "    ########################################\n",
    "    expected_result = np.dot(v, M)\n",
    "    actual_result = jnp.dot(v, M)\n",
    "\n",
    "    assert jnp.allclose(expected_result, actual_result)\n",
    "    print(\"Dot product passed\")\n",
    "    \n",
    "\n",
    "def is_even_filter():\n",
    "    to_filter_np = np.asarray([1, 2, 3, 5, 10, 20])\n",
    "    expected_result = to_filter_np[to_filter_np % 2 == 0]\n",
    "\n",
    "    to_filter_jnp = jnp.asarray([1, 2, 3, 5, 10, 20])\n",
    "    ########################################\n",
    "    # Your code here\n",
    "    ########################################\n",
    "    actual_result = to_filter_jnp[to_filter_jnp % 2 == 0]\n",
    "\n",
    "    assert jnp.allclose(expected_result, actual_result)\n",
    "    print(\"is_even_filter passed\")\n",
    "\n",
    "def top_n_of_norm_squared():\n",
    "    M = jnp.asarray(np.random.rand(10, 5))\n",
    "    TOP_N = 5\n",
    "    \n",
    "    expected_result = np.sort(np.linalg.norm(M @ M.T, axis=1))[::-1][:TOP_N]\n",
    "\n",
    "    ########################################\n",
    "    # Your code here\n",
    "    ########################################\n",
    "    actual_result = jnp.sort(jnp.linalg.norm(M @ M.T, axis=1))[::-1][:TOP_N]\n",
    "\n",
    "    assert jnp.allclose(expected_result, actual_result)\n",
    "    print(\"top_n_of_norm_squared passed\")\n",
    "\n",
    "dot_product()\n",
    "is_even_filter()\n",
    "top_n_of_norm_squared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb6f70-a4ed-4c60-a885-afcd3fcf42a6",
   "metadata": {},
   "source": [
    "# Returning to the LIF Example\n",
    "\n",
    "<img src=\"../assets/lif_formulation.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "We now return to the LIF model we discussed earlier. Simply put, we want to replace the np.X calls with jnp.X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad8c04-4574-47d1-a10f-1d4fe97b1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import (\n",
    "    _dt,\n",
    "    _t_max,\n",
    "    _tau_m,\n",
    "    _V_reset,\n",
    "    _V_thresh,\n",
    "    _R,\n",
    "    num_simulations\n",
    ")\n",
    "\n",
    "\n",
    "with open('weights.npy', 'rb') as f:\n",
    "    W = np.load(f)\n",
    "\n",
    "# Initial conditions\n",
    "n_neurons = len(W)# Number of neurons in the network\n",
    "_V = jnp.ones(n_neurons) * _V_reset  # Initial potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f14d01-6637-4daf-850f-7db1e38b7e56",
   "metadata": {},
   "source": [
    "# Type Definitions for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7932809-ea42-4e5d-9b59-838cd0ac2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor3D: TypeAlias = jnp.ndarray\n",
    "Mat: TypeAlias = jnp.ndarray\n",
    "Vec: TypeAlias = jnp.ndarray "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2191f-f7ae-4c72-b901-65e5344e2784",
   "metadata": {},
   "source": [
    "# Run the Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4fcc6-6cab-4f63-9672-9b4ea02d13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(\n",
    "    W: Mat,\n",
    "    V: Vec,\n",
    "\n",
    "    # Neuron Parameters\n",
    "    tau_m: float,\n",
    "    v_reset: float,\n",
    "    v_thresh: float,\n",
    "    membr_R: float,\n",
    "\n",
    "    # How long do we run for? \n",
    "    t_max: float,\n",
    "    dt: float, \n",
    "\n",
    "):\n",
    "    # Simulation\n",
    "\n",
    "    spike_train = []\n",
    "    for i, t in enumerate(np.arange(0, t_max, dt)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        fired = V >= v_thresh\n",
    "        V = jnp.where(fired, v_reset, V)\n",
    "        \n",
    "        # Record spike times\n",
    "        spike_train.append(fired)\n",
    "    \n",
    "        # Update voltages\n",
    "        I_syn = W.dot(spike_train[-1])  # Synaptic current from spikes\n",
    "        dV = (dt / tau_m) * (-V + v_reset + membr_R * I_syn)\n",
    "        V = V + dV\n",
    "    \n",
    "        # No self-inputs; neurons cannot spike themselves in this timestep\n",
    "        V = jnp.where(fired, v_reset, V)\n",
    "    return spike_train\n",
    "\n",
    "time_arr = []\n",
    "for i in range(num_simulations):\n",
    "    start = time.time()\n",
    "    spike_train = run_simulation(\n",
    "        W,\n",
    "        _V,\n",
    "        _tau_m, _V_reset, _V_thresh, _R,\n",
    "        _t_max, _dt\n",
    "    )\n",
    "    np.asarray(spike_train)\n",
    "    end = time.time()\n",
    "    print(f\"Iteration {i} took: {end - start} seconds\")\n",
    "    time_arr.append(end - start)\n",
    "\n",
    "print(f\"Average Time: {np.mean(time_arr)}\")\n",
    "print(f\"S.Dev Time: {np.std(time_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83581c3a-aaee-4a73-82bf-5744426283fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
