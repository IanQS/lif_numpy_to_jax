{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea93709-b1aa-4aeb-b4e8-2277e04eb5dc",
   "metadata": {},
   "source": [
    "# Einstein Notation\n",
    "\n",
    "## Goals:\n",
    "\n",
    "- understand the various matrix operations `einsum` can replace\n",
    "\n",
    "## Concepts:\n",
    "\n",
    "- `einsum`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bdd7db-d0c9-4545-93b6-1c0092e8cc24",
   "metadata": {},
   "source": [
    "# Matrix Operations:\n",
    "\n",
    "- dot product\n",
    "- matrix-vector\n",
    "- matrix-matrix\n",
    "- hadamard product\n",
    "- outer product\n",
    "- batched matrix mult\n",
    "- transpose\n",
    "- summing along an axis after multiplication\n",
    "- trace\n",
    "- diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528f55c5-8f79-4022-91fe-adbb070ee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90280239-5cbd-4464-99ee-4ee52e5efab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEAT = 10\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "v1 = jnp.asarray(np.random.rand(NUM_FEAT))\n",
    "v2 = jnp.asarray(np.random.rand(NUM_FEAT))\n",
    "\n",
    "M1 = jnp.asarray(np.random.rand(NUM_FEAT, NUM_SAMPLES))\n",
    "M2 = jnp.asarray(np.random.rand(NUM_SAMPLES, NUM_FEAT))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_CHANNELS = 3\n",
    "HEIGHT = 32\n",
    "WIDTH = 32\n",
    "\n",
    "LEARNED_FILTERS = 5\n",
    "images = jnp.asarray(np.random.rand(BATCH_SIZE, HEIGHT, WIDTH, NUM_CHANNELS))\n",
    "learned_kernels = jnp.asarray(np.random.rand(LEARNED_FILTERS, HEIGHT, WIDTH, NUM_CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adb9ae-b029-4051-8810-b2d55ffe18bf",
   "metadata": {},
   "source": [
    "# Vector-Vector dot product\n",
    "\n",
    "which returns a scalar. This is equivalent to doing a hadamard-then-sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3807b764-f895-4f40-ba7e-7b70c094caa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All vector dot-products equivalent? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 16:00:19.246122: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.4.131). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# vector-vector dot-product\n",
    "def vv_dot():\n",
    "    res = jnp.dot(v1, v2)\n",
    "    res2 = v1 @ v2\n",
    "    res3 = jnp.matmul(v1, v2)\n",
    "    \n",
    "    res_ein = jnp.einsum(\"j, j ->\", v1, v2)\n",
    "    print(\"All vector dot-products equivalent?\",\n",
    "        jnp.all(res == res2 == res3 == res_ein)\n",
    "    )\n",
    "\n",
    "vv_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1301217-0353-42b4-8c6e-38d2fde5b437",
   "metadata": {},
   "source": [
    "# Outer Product\n",
    "\n",
    "Let \n",
    "\n",
    "$$u \\in \\mathbb{R}^m$$\n",
    "\n",
    "and \n",
    "\n",
    "$$v \\in \\mathbb{R}^n$$\n",
    "\n",
    "then the dot-product is\n",
    "\n",
    "$$(u \\otimes v) \\in \\mathbb{R}^{m, n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ded86c-9e62-4495-beab-eab0a9f58e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer-products equivalent? True\n"
     ]
    }
   ],
   "source": [
    "# vector-vector outer-product\n",
    "\n",
    "def vv_outer():\n",
    "    res = jnp.outer(v1, v2)\n",
    "    \n",
    "    res_ein = jnp.einsum(\"i, j ->ij\", v1, v2)\n",
    "    print(\n",
    "        \"Outer-products equivalent?\",\n",
    "        jnp.allclose(res, res_ein))\n",
    "\n",
    "vv_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19daa8b-a1fe-4302-a138-cffa372c671c",
   "metadata": {},
   "source": [
    "# Matrix-Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e3b4e2-744f-48a3-a816-e62b4b99999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix-mult equivalent? True\n",
      "Matrix-mult then sum-rows equivalent? True\n",
      "Matrix-mult then sum-cols equivalent? True\n"
     ]
    }
   ],
   "source": [
    "def mm_mult():\n",
    "    # Matrix mult\n",
    "    res_mm = jnp.dot(M1, M2)\n",
    "    res_ein_mm = np.einsum(\"ij, jl ->il\", M1, M2)\n",
    "    print(\"Matrix-mult equivalent?\",jnp.allclose(res_mm, res_ein_mm))\n",
    "\n",
    "    # Matrix mult then sum along the 0-th axis\n",
    "    res_mm = np.sum(jnp.dot(M1, M2), axis=0)\n",
    "    res_ein_mm = np.einsum(\"ij, jl ->l\", M1, M2)\n",
    "    print(\"Matrix-mult then sum-rows equivalent?\",jnp.allclose(res_mm, res_ein_mm))\n",
    "\n",
    "    # Matrix mult then sum along the 1st axis\n",
    "    res_mm = np.sum(jnp.dot(M1, M2), axis=1)\n",
    "    res_ein_mm = np.einsum(\"ij, jl ->i\", M1, M2)\n",
    "    print(\"Matrix-mult then sum-cols equivalent?\",jnp.allclose(res_mm, res_ein_mm))\n",
    "\n",
    "mm_mult()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ca0fa-8e46-44e0-9127-bafb5aaee8c0",
   "metadata": {},
   "source": [
    "# Misc. Operations\n",
    "\n",
    "Some other operations that are useful involve taking:\n",
    "\n",
    "- the trace, the sum of the diagonals\n",
    "- the diagonals themselves\n",
    "- transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd0ccc9c-7c64-49fa-962a-2ddf91b39b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-square matrix trace equivalent? False\n",
      "Square matrix trace equivalent? True\n",
      "Matrix Transpose equivalent? True\n",
      "\n",
      " dimensions in single operand for collapsing index 'i' don't match (10 != 100)\n",
      "Diag on square matrix equivalent? True\n"
     ]
    }
   ],
   "source": [
    "def trace():\n",
    "\n",
    "    #############################################\n",
    "    # Note: the einsum trace is only defined when the matrix is square\n",
    "    #############################################\n",
    "    print(\n",
    "        \"Non-square matrix trace equivalent?\",\n",
    "        jnp.allclose(\n",
    "            jnp.trace(M1), \n",
    "            np.einsum(\"ij->\", M1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Square matrix trace equivalent?\",\n",
    "        jnp.allclose(\n",
    "            jnp.trace(M1 @ M1.T), \n",
    "            np.einsum(\"ii->\", M1 @ M1.T)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def transpose():\n",
    "    #############################################\n",
    "    # Transpose\n",
    "    print(\n",
    "        \"Matrix Transpose equivalent?\", jnp.allclose(\n",
    "        M1.T, \n",
    "        np.einsum(\"ij -> ji\", M1)\n",
    "    ))\n",
    "\n",
    "def diagonal():\n",
    "    #############################################\n",
    "    # Diagonal\n",
    "    # Note: the einsum diagonal is only defined when the matrix is square\n",
    "    try:\n",
    "        print(\n",
    "            \"Diag on non-square matrix equivalent?\",\n",
    "            jnp.allclose(\n",
    "            jnp.diag(M1), \n",
    "            np.einsum(\"ii -> i\", M1)\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(\"\\n\", e)\n",
    "        # We get the expected result\n",
    "        print(\n",
    "            \"Diag on square matrix equivalent?\",\n",
    "            jnp.allclose(\n",
    "            jnp.diag(M1 @ M1.T), \n",
    "            np.einsum(\"ii -> i\", M1 @ M1.T)\n",
    "        ))\n",
    "\n",
    "\n",
    "trace()\n",
    "transpose()\n",
    "diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc5192-46a6-4637-9d46-dc2749380f26",
   "metadata": {},
   "source": [
    "# Higher-dimensional operations\n",
    "\n",
    "Scenario: we have learned some filters and want to apply those filters to an image.\n",
    "\n",
    "Another good example, which we do not cover, is calculating attention in LLMs. See [Einsum is All you Need - Einstein Summation in Deep Learning#3.2 Attention](https://rockt.github.io/2018/04/30/einsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac755ec-d46b-4b6a-805b-bd4a227f579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Contraction Equivalent? True\n"
     ]
    }
   ],
   "source": [
    "def tensor_contraction():\n",
    "    #############################################\n",
    "    # Apply our learned filters (T2) to our data (T1)\n",
    "    #############################################\n",
    "    flattened_images = images.reshape(BATCH_SIZE, HEIGHT * WIDTH * NUM_CHANNELS)\n",
    "    \n",
    "    # Flatten height, width, and channels for T2, but need to transpose to match the matrix multiplication requirements\n",
    "    flattened_kernel = learned_kernels.reshape(LEARNED_FILTERS, HEIGHT * WIDTH * NUM_CHANNELS).transpose()\n",
    "    \n",
    "    # Matrix multiplication\n",
    "    res_tc = jnp.matmul(flattened_images, flattened_kernel).reshape(BATCH_SIZE, LEARNED_FILTERS)\n",
    "\n",
    "    # Einsum\n",
    "    res_ein = jnp.einsum('bhwc,fhwc->bf', images, learned_kernels)\n",
    "\n",
    "    print(\n",
    "        \"Tensor Contraction Equivalent?\",\n",
    "        jnp.allclose(\n",
    "            res_tc,\n",
    "            res_ein\n",
    "    ))\n",
    "\n",
    "tensor_contraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28220f6-9e36-4fe0-83f0-e9675cc2946e",
   "metadata": {},
   "source": [
    "# Gaussian PDF\n",
    "\n",
    "One of the steps involves calculating \n",
    "\n",
    "$$(x - \\mu)^T\\Sigma(x - \\mu)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557fdab7-3a4f-4a54-8ef0-e388ff491d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponent calculations equivalent?:  [ True  True  True]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian_pdf(x, mu, Sigma):\n",
    "    # Dimensions of the data\n",
    "    k = mu.shape[0]\n",
    "    \n",
    "    # Calculate determinant and inverse of the covariance matrix\n",
    "    Sigma_det = jnp.linalg.det(Sigma)\n",
    "    Sigma_inv = jnp.linalg.inv(Sigma)\n",
    "    \n",
    "    # Calculate the normalization factor\n",
    "    normalization_factor = 1 / jnp.sqrt((2 * jnp.pi) ** k * Sigma_det)\n",
    "    x_mu = x - mu\n",
    "    ###############################################################\n",
    "    exponent1 = -0.5 * jnp.sum(x_mu @ Sigma_inv * x_mu, axis=1)\n",
    "    exponent2 = -0.5 * jnp.einsum('ij,jj,ij->i', x_mu, Sigma_inv, x_mu)\n",
    "    print(\"Exponent calculations equivalent?: \", exponent1 == exponent2)\n",
    "    ###############################################################\n",
    "    \n",
    "    # Compute the Gaussian PDF\n",
    "    return normalization_factor * jnp.exp(exponent2)\n",
    "\n",
    "# Example usage\n",
    "mu = np.array([0, 0])  # Mean vector\n",
    "Sigma = np.array([[1, 0], [0, 1]])  # Covariance matrix\n",
    "x = np.array([[1, 1], [2, 2], [3, 3]])  # Point to evaluate the PDF\n",
    "\n",
    "pdf_value = gaussian_pdf(x, mu, Sigma)\n",
    "\n",
    "print(jnp.allclose(\n",
    "    pdf_value, \n",
    "    stats.multivariate_normal.pdf(x, mu, Sigma)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855acc7-05de-45e5-a195-b9d1e0aa2a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
