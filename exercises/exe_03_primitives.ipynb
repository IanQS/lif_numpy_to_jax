{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2d138c-105a-4679-99d9-704c47a27570",
   "metadata": {},
   "source": [
    "# Jax's Loops\n",
    "\n",
    "By the end of this lesson, you'll be able to articulate why and when you want to use `jax`'s native `while_loop`, `fori_loop`, and `scan` over python's native loops. In the process, you'll learn how to read haskell-like type signatures, which will be useful as you explore the `jax` library further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0968a7d8-32b5-4306-a423-2bd84d003f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import TypeAlias\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from tqdm.notebook import tqdm\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbad8c04-4574-47d1-a10f-1d4fe97b1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import (\n",
    "    _dt,\n",
    "    _t_max,\n",
    "    _tau_m,\n",
    "    _V_reset,\n",
    "    _V_thresh,\n",
    "    _R,\n",
    "    num_simulations\n",
    ")\n",
    "\n",
    "with open('weights.npy', 'rb') as f:\n",
    "    W = np.load(f)\n",
    "\n",
    "# Initial conditions\n",
    "n_neurons = len(W)# Number of neurons in the network\n",
    "_V = jnp.ones(n_neurons) * _V_reset  # Initial potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f14d01-6637-4daf-850f-7db1e38b7e56",
   "metadata": {},
   "source": [
    "# Type Definitions for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7932809-ea42-4e5d-9b59-838cd0ac2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor3D: TypeAlias = jnp.ndarray\n",
    "Mat: TypeAlias = jnp.ndarray\n",
    "Vec: TypeAlias = jnp.ndarray "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0f684-982b-4575-a58b-ca7f8e8ad4e2",
   "metadata": {},
   "source": [
    "# Haskell-like signatures\n",
    "\n",
    "type signatures are a great way to abstractly understand functions and what they do. Let's walk through a few examples:\n",
    "\n",
    "## Examples to work through\n",
    "\n",
    "```haskell\n",
    "map :: (a -> b) -> [a] -> [b]\n",
    "```\n",
    "\n",
    "```haskell\n",
    "sum :: a => [a] -> a\n",
    "```\n",
    "\n",
    "```haskell\n",
    "(++) :: [a] -> [a] -> [a]\n",
    "```\n",
    "\n",
    "```haskell\n",
    "filter :: (a -> Bool) -> a -> a \n",
    "```\n",
    "\n",
    "## The Jax Functions we will be covering:\n",
    "\n",
    "```haskell\n",
    "while_loop :: (a -> Bool) -> (a -> a) -> a -> a\n",
    "```\n",
    "\n",
    "```haskell\n",
    "fori_loop :: Int -> Int -> ((Int, a) -> a) -> a -> a\n",
    "```\n",
    "\n",
    "```haskell\n",
    "scan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767ec5f-e0e2-4c68-af6b-a4b520017951",
   "metadata": {},
   "source": [
    "## Jax's Loops\n",
    "\n",
    "```python\n",
    "def while_loop(cond_fun, body_fun, init_val):\n",
    "    val = init_val\n",
    "    while cond_fun(val):\n",
    "        val = body_fun(val)\n",
    "    return val\n",
    "\n",
    "def fori_loop(lower, upper, body_fun, init_val):\n",
    "    val = init_val\n",
    "    for i in range(lower, upper):\n",
    "        val = body_fun(i, val)\n",
    "    return val\n",
    "\n",
    "def scan(f, init, xs, length=None):\n",
    "    if xs is None:\n",
    "        xs = [None] * length\n",
    "    carry = init\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        carry, y = f(carry, x)\n",
    "        ys.append(y)\n",
    "  return carry, np.stack(ys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43bedd-3218-4a2b-ad14-3ff9b91fbcdd",
   "metadata": {},
   "source": [
    "## Jax's Loops\n",
    "\n",
    "```python\n",
    "def while_loop(cond_fun, body_fun, init_val):\n",
    "    val = init_val\n",
    "    while cond_fun(val):\n",
    "        val = body_fun(val)\n",
    "    return val\n",
    "\n",
    "def fori_loop(lower, upper, body_fun, init_val):\n",
    "    val = init_val\n",
    "    for i in range(lower, upper):\n",
    "        val = body_fun(i, val)\n",
    "    return val\n",
    "\n",
    "def scan(f, init, xs, length=None):\n",
    "    if xs is None:\n",
    "        xs = [None] * length\n",
    "    carry = init\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        carry, y = f(carry, x)\n",
    "        ys.append(y)\n",
    "  return carry, np.stack(ys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b20fe3-eabd-4fc1-8361-681a1f0d9b42",
   "metadata": {},
   "source": [
    "# Looping in Jax\n",
    "\n",
    "As mentioned before, you probably don't want to `jit` a function that has a native python `for-loop` in it as this increases your compilation time. Thankfully, `jax` provides:\n",
    "\n",
    "- [jax.lax.while_loop](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.while_loop.html#jax.lax.while_loop)\n",
    "- [jax.lax.fori_loop](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.fori_loop.html)\n",
    "\n",
    "to circumvent this issue. \n",
    "\n",
    "Note: we don't necessarily see a speedup in runtime (although that can happen). The primary advantage of using these jax functions is that the compilation time can be reduced. Below we have an example of the jax `fori_loop` in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc87931-87c7-4585-a1e5-b16d97e1461e",
   "metadata": {},
   "source": [
    "# Lotka Volterra\n",
    "\n",
    "Let's consider a simple example using the following equation:\n",
    "\n",
    "![](../assets/lotka_volterra.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9ed5b3-523d-4055-b743-246812c1611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array(0.83927435, dtype=float32, weak_type=True), Array(4.9796286, dtype=float32, weak_type=True), Array([[10.        ,  5.        ],\n",
      "       [ 9.1       ,  5.3       ],\n",
      "       [ 8.171801  ,  5.5703    ],\n",
      "       [ 7.249923  ,  5.802682  ],\n",
      "       [ 6.3646545 ,  5.991265  ],\n",
      "       [ 5.539473  ,  6.1329374 ],\n",
      "       [ 4.7898855 ,  6.227352  ],\n",
      "       [ 4.1236405 ,  6.276541  ],\n",
      "       [ 3.5419528 ,  6.2843018 ],\n",
      "       [ 3.0412197 ,  6.2555165 ],\n",
      "       [ 2.6147778 ,  6.19554   ],\n",
      "       [ 2.2544048 ,  6.109718  ],\n",
      "       [ 1.9514382 ,  6.003067  ],\n",
      "       [ 1.6975118 ,  5.8800907 ],\n",
      "       [ 1.4849771 ,  5.7447023 ],\n",
      "       [ 1.3070946 ,  5.6002216 ],\n",
      "       [ 1.1580743 ,  5.449413  ],\n",
      "       [ 1.0330294 ,  5.2945447 ],\n",
      "       [ 0.92788583,  5.137457  ],\n",
      "       [ 0.83927435,  4.9796286 ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import numpy as np\n",
    "\n",
    "@jax.jit\n",
    "def lotka_volterra_step(state, params):\n",
    "    x, y = state\n",
    "    alpha, beta, gamma, delta, dt = params\n",
    "    \n",
    "    dxdt = alpha * x - beta * x * y\n",
    "    dydt = delta * x * y - gamma * y\n",
    "    \n",
    "    x_new = x + dxdt * dt\n",
    "    y_new = y + dydt * dt\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "@jax.jit\n",
    "def body_func(i, val):\n",
    "    x, y, trajectory = val\n",
    "    x, y = lotka_volterra_step((x, y), params)\n",
    "    trajectory = trajectory.at[i].set([x, y])\n",
    "    return x, y, trajectory\n",
    "\n",
    "# Parameters\n",
    "alpha = 1.1\n",
    "beta = 0.4\n",
    "gamma = 0.4\n",
    "delta = 0.1\n",
    "dt = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "# Initial populations\n",
    "x_prev = 10.0\n",
    "y_prev = 5.0\n",
    "\n",
    "params = (alpha, beta, gamma, delta, dt)\n",
    "trajectory = jnp.zeros((num_steps, 2))\n",
    "trajectory = trajectory.at[0].set([x_prev, y_prev])\n",
    "\n",
    "\"\"\"\n",
    "TODO: convert the following explicit for-loop into a jax `fori_loop`\n",
    "    fori_loop :: Int -> Int -> ((Int, a) -> a) -> a -> a\n",
    "\"\"\"\n",
    "\n",
    "trajectory = jax.lax.fori_loop(1, num_steps, body_func,  (x_prev, y_prev, trajectory))\n",
    "print(trajectory)\n",
    "# for i in range(1, num_steps):\n",
    "#     x_new, y_new, trajectory = body_func(i, (x_prev, y_prev, trajectory))\n",
    "#     x_prev, y_prev = x_new, y_new\n",
    "# print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb87eb-ca25-4a58-893d-f3e3c29d5f1b",
   "metadata": {},
   "source": [
    "# Jax Scan\n",
    "\n",
    "As we saw in the type signature, using the `scan` allows us to carry values between iterations. This is clearly ideal for our application and we should thus use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e369b-1270-4438-96ca-0dba39067fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def run_step(v_prev, v_thresh, v_reset, W, tau_m, dt, membr_R):\n",
    "    spiked = v_prev >= v_thresh\n",
    "    V = jnp.where(spiked, v_reset, v_prev)\n",
    "\n",
    "    I_syn = W @ spiked.astype(jnp.float32)  # Synaptic current from spikes\n",
    "    dV = (dt / tau_m) * (-V + v_reset + membr_R * I_syn)\n",
    "    V = V + dV\n",
    "\n",
    "    V = jnp.where(spiked, v_reset, V)\n",
    "    return V, spiked\n",
    "\n",
    "@jax.jit\n",
    "def scan_step(carry, _):\n",
    "\n",
    "    (V, v_thresh, v_reset, W, tau_m, dt, membr_R) = carry\n",
    "    new_V, spike = run_step(V, v_thresh, v_reset, W, tau_m, dt, membr_R)\n",
    "    return (new_V, v_thresh, v_reset, W, tau_m, dt, membr_R), spike\n",
    "\n",
    "def run_simulation(W, V, tau_m, v_reset, v_thresh, membr_R, t_max, dt):\n",
    "    \"\"\"\n",
    "    TODO: \n",
    "        Implement the scan step!\n",
    "        \n",
    "    \"\"\"\n",
    "    num_steps = int(t_max / dt)\n",
    "    # Run the scan over the number of time steps\n",
    "    final_V, accum_spikes = ...\n",
    "    return accum_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8bcfe-aec0-4f4e-8312-099040e23d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "No need to change the code below\n",
    "\"\"\"\n",
    "\n",
    "time_arr = []\n",
    "for i in range(num_simulations):\n",
    "    start = time.time()\n",
    "    spike_train = run_simulation(\n",
    "        jnp.asarray(W),\n",
    "        _V,\n",
    "        _tau_m, _V_reset, _V_thresh, _R,\n",
    "        _t_max, _dt\n",
    "    )\n",
    "    end = time.time()\n",
    "    np.asarray(spike_train)\n",
    "    print(f\"Iteration {i} took: {end - start} seconds\")\n",
    "    time_arr.append(end - start)\n",
    "\n",
    "print(f\"Average Time: {np.mean(time_arr)}\")\n",
    "print(f\"S.Dev Time: {np.std(time_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32d2b7-3095-450c-ad91-0c0db029421a",
   "metadata": {},
   "source": [
    "# Further Exercises: \n",
    "\n",
    "## 1) Convert the example Lotka Volterra equation above to use a `while_loop`\n",
    "\n",
    "## 2) Work through [extras_namedtuple.ipynb](./extras_namedtuple.ipynb) \n",
    "\n",
    "which teaches you some best practices to learn how you can keep your code clean using python's `namedtuples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa24ff-0eca-4b65-8b5f-77f62457be07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
