{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jax's RNG\n",
    "\n",
    "## Lesson Goals:\n",
    "\n",
    "By the end of this lesson you will understand why `jax`'s [random module](https://jax.readthedocs.io/en/latest/jax.random.html) is structured the way it is. For additional (external) information read [Jax's Pseudorandom numbers](https://jax.readthedocs.io/en/latest/random-numbers.html) documentation.\n",
    "\n",
    "## Core Concepts:\n",
    "\n",
    "- `rng`\n",
    "- `samplers`\n",
    "\n",
    "## Concepts In action:\n",
    "\n",
    "TODO\n"
   ],
   "id": "29fc9efc95b9620a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "\n",
    "It's easy to conflate library-level determinism and GPU-level determinism. Jax and how you generate random samples is deterministic\n",
    "\n",
    "> `jax.random.split()` is a deterministic function that converts one key into several independent (in the pseudorandomness sense) keys.\n",
    "\n",
    "as per [random-numbers](https://jax.readthedocs.io/en/latest/random-numbers.html), but depending on how you've configured your system environment, your GPU might be causing non-determinism. See [one of the creators of Jax's response to a question about determinism](https://github.com/google/jax/discussions/10674)\n"
   ],
   "id": "5ad4a71a1def8a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Foreword\n",
    "\n",
    "## Numpy\n",
    "\n",
    "In [lesson 2](./exe_02_jit.ipynb) we had briefly discussed state and functional programming, and we showed how using stateful programs can lead to bugs or unexpected results (remember the shopping cart?)\n",
    "\n",
    "Well, it turns out that when you run code like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "np.random.rand()\n",
    "```\n",
    "\n",
    "you're actually relying on global state of the program. This reliance on global state leads to a whole slew of problems e.g. when you're distributing your program across multiple machines, processes or threads. \n",
    "Randomness has been a pit trap for many a machine learner (see the [The Pit of Reproducibility](#the-pit-of-reproducibility) for examples). In numpy, every subsequent call to a sampler changes the state of the world, which means that we cannot guarantee\n",
    "that the same number will be generated for a fixed program. \n"
   ],
   "id": "b21e32b95ab1095b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Jax\n",
    "\n",
    "How does Jax handle the issue of state, then? Well, it relies on:\n",
    "\n",
    "1) explicitly passing in a random seed/key every time you sample (sounds cumbersome, but it's really not!)\n",
    "2) "
   ],
   "id": "2f8d2a6bcbcc1584"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Pit of Reproducibility\n",
    "\n",
    "- [TensorFlow results are not reproducible despite using tf.random.set_seed](https://stackoverflow.com/questions/75850086/tensorflow-results-are-not-reproducible-despite-using-tf-random-set-seed)\n",
    "- [Stackoverflow answer about using an experimental tensorflow function](https://stackoverflow.com/a/71311207/24169564), [tf.config.experimental.enable_op_determinism](https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism)\n",
    "- [Nvidia guide on Pytorch and Tensorflow Reproducibility](https://github.com/NVIDIA/framework-reproducibility)\n"
   ],
   "id": "745aab4929a43f10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
