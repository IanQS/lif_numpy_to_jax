{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2d138c-105a-4679-99d9-704c47a27570",
   "metadata": {},
   "source": [
    "# Jax jit\n",
    "\n",
    "## Lesson Goals:\n",
    "\n",
    "By the end of this lesson, you will know how to use the `jit`, how to accurately time computations using `jit`-ted functions, and how to identify where to `jit` things. In the process, we will quickly discuss functional programming and why functional programming is useful for speeding up computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597a69c0-35b4-4966-b6de-16757f3c078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56579c4-ee73-4e7b-b428-9505f7191d4c",
   "metadata": {},
   "source": [
    "# Functional Programming?\n",
    "\n",
    "Functional programming is many things, but for the purposes of this tutorial, it is a form of programming without side-effects. Python is not a functional programming language, but you may have heard of others such as `haskell`, `ocaml`, or `erlang`.\n",
    "\n",
    "The most common form of side-effects involves modifying some internal state. Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a049b0b1-adc1-44ec-9f4d-7b22af832ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShoppingCart(['banana'])\n",
      "ShoppingCart(['banana', 'apple'])\n",
      "**********\n",
      "FunctionalShoppingCart([])\n",
      "FunctionalShoppingCart(['apple'])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "class ShoppingCart:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)  # Side effect: modifying internal state\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ShoppingCart({self.items})\"\n",
    "\n",
    "cart = ShoppingCart()\n",
    "cart.add_item(\"banana\")\n",
    "print(cart)\n",
    "cart.add_item(\"apple\")\n",
    "print(cart)\n",
    "print(\"*\" * 10)\n",
    "\n",
    "class FunctionalShoppingCart:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def add_item(self, item):\n",
    "        new_cart = FunctionalShoppingCart()\n",
    "        all_items = copy.deepcopy(self.items)\n",
    "        all_items.append(item)\n",
    "        new_cart.items = all_items\n",
    "        return new_cart\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FunctionalShoppingCart({self.items})\"\n",
    "\n",
    "func_cart = FunctionalShoppingCart()\n",
    "func_cart.add_item(\"banana\")  # <- The banana was not added!\n",
    "print(func_cart)\n",
    "func_cart = func_cart.add_item(\"apple\")\n",
    "print(func_cart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f26b1-de6a-499a-a617-8704087de88c",
   "metadata": {},
   "source": [
    "## Functional Programming:\n",
    "\n",
    "Okay, but how is this relevant? Well, functional programming allows for:\n",
    "\n",
    "- predictable behavior: compilers can more easily optimize your code\n",
    "- immutability: the data cannot be modified, so all threads/ processes just grab a copy of the original data and process it async."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe45c7-c7d5-4e7c-8e7c-c1c9aaa83669",
   "metadata": {},
   "source": [
    "# Jax's JIT: Supercharged functions\n",
    "\n",
    "A `jit` is a just-in-time compilation of your code. `Python` is famously slow because, among other things, the code is interpreted i.e. at run-time, the interpreter has to decide what to do. Languages like `C++` are `Rust` are compiled so at run-time, the code is just... run.\n",
    "\n",
    "So, by compiling out Jax code via the `jit`, we can accelerate our programs. Assuming the numerical computation is the bottleneck, as is often the case in ML tasks, this means that we have sped up the slowest part of our program.\n",
    "\n",
    "## Where does functional programming come in? \n",
    "\n",
    "FP makes it easier for the `jit` compiler to speed up the code. It can do things like:\n",
    "\n",
    "- function inlining: the function call is replaced by the function itself\n",
    "\n",
    "- loop fusion/elimination/unrolling: by removing dependencies between calls, jax can \n",
    "\n",
    "- memoization: jax can cache results for particular inputs and return those if it sees those particular inputs again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe5e698-198f-4028-8723-c9ea6ce3cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version\n",
      "25.5 ms ± 2.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Jax Non-Jit version\n",
      "5.26 ms ± 50.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Jax Jitted version\n",
      "4.67 ms ± 311 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "input_arr = np.random.rand(1000, 1000)\n",
    "\n",
    "def func_np(m):\n",
    "    mask = m > 0.5\n",
    "    m = np.where(mask, m**2, np.sqrt(m))\n",
    "    return m @ m\n",
    "\n",
    "print(\"Numpy version\")\n",
    "%timeit func_np(input_arr)\n",
    "\n",
    "input_arr_j = jnp.asarray(input_arr)\n",
    "\n",
    "def func_jax(m):\n",
    "    mask = m > 0.5\n",
    "    mod_m = jnp.where(mask, m**2, jnp.sqrt(m))\n",
    "    return mod_m @ mod_m\n",
    "\n",
    "print(\"Jax Non-Jit version\")\n",
    "%timeit func_jax(input_arr_j)\n",
    "\n",
    "jitted_func = jax.jit(func_jax)\n",
    "\n",
    "print(\"Jax Jitted version\")\n",
    "%timeit jitted_func(input_arr_j).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a26b4f-a812-4739-979f-a2fab006200e",
   "metadata": {},
   "source": [
    "## Quick Aside: Benchmarking in Jax\n",
    "\n",
    "The astute would have noticed the `.block_until_ready()` function call. What gives? Well, jax returns a future to prevent blocking the main python thread. So, to get accurate timings we had to use the `.block_until_ready()`. To ensure that you get accurate timings when benchmarking you can:\n",
    "\n",
    "- use `.block_until_ready()`\n",
    "- convert the `jnp.array` into `np.array` to wait for the future\n",
    "- print the `jnp.array`\n",
    "\n",
    "For more information check out: [Jax Async Dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbad8c04-4574-47d1-a10f-1d4fe97b1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from hyperparameters import (\n",
    "    _dt,\n",
    "    _t_max,\n",
    "    _tau_m,\n",
    "    _V_reset,\n",
    "    _V_thresh,\n",
    "    _R,\n",
    "    num_simulations\n",
    ")\n",
    "\n",
    "\n",
    "with open('weights.npy', 'rb') as f:\n",
    "    W = np.load(f)\n",
    "\n",
    "# Initial conditions\n",
    "n_neurons = len(W)# Number of neurons in the network\n",
    "_V = jnp.ones(n_neurons) * _V_reset  # Initial potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f14d01-6637-4daf-850f-7db1e38b7e56",
   "metadata": {},
   "source": [
    "# Type Definitions for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7932809-ea42-4e5d-9b59-838cd0ac2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor3D: TypeAlias = jnp.ndarray\n",
    "Mat: TypeAlias = jnp.ndarray\n",
    "Vec: TypeAlias = jnp.ndarray "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2191f-f7ae-4c72-b901-65e5344e2784",
   "metadata": {},
   "source": [
    "# Run the Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b4fcc6-6cab-4f63-9672-9b4ea02d13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 took: 18.002535820007324 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_simulations):\n\u001b[1;32m     54\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 55\u001b[0m     spike_train \u001b[38;5;241m=\u001b[39m run_simulation(\n\u001b[1;32m     56\u001b[0m         W,\n\u001b[1;32m     57\u001b[0m         _V,\n\u001b[1;32m     58\u001b[0m         _tau_m, _V_reset, _V_thresh, _R,\n\u001b[1;32m     59\u001b[0m         t_max\u001b[38;5;241m=\u001b[39m_t_max, dt\u001b[38;5;241m=\u001b[39m_dt\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(spike_train)\n\u001b[1;32m     62\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m, in \u001b[0;36mrun_simulation\u001b[0;34m(W, V, tau_m, v_reset, v_thresh, membr_R, t_max, dt)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     V, spiked\u001b[38;5;241m=\u001b[39m run_step(V, v_thresh, v_reset, W, tau_m, dt, membr_R)\n\u001b[1;32m     49\u001b[0m     spike_train\u001b[38;5;241m.\u001b[39mappend(spiked)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spike_train\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def run_step(\n",
    "    v_prev,\n",
    "    v_thresh,\n",
    "    v_reset,\n",
    "\n",
    "    W,\n",
    "    tau_m,\n",
    "    dt,\n",
    "    membr_R):\n",
    "    \n",
    "    spiked = v_prev >= v_thresh\n",
    "    V = jnp.where(spiked, v_reset, v_prev)\n",
    "    I_syn = W.dot(spiked)\n",
    "\n",
    "    dV = (dt / tau_m) * (-V + v_reset + membr_R * I_syn)\n",
    "    V = V + dV\n",
    "    V = jnp.where(spiked, v_reset, V)\n",
    "    return V, spiked\n",
    "\n",
    "def run_simulation(\n",
    "    W: Mat,\n",
    "    V: Vec,\n",
    "\n",
    "    # Neuron Parameters\n",
    "    tau_m: float,\n",
    "    v_reset: float,\n",
    "    v_thresh: float,\n",
    "    membr_R: float,\n",
    "\n",
    "    # How long do we run for? \n",
    "    t_max: float,\n",
    "    dt: float, \n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO while keeping the function signature the same, abstract out \n",
    "        the contents of the for-loop into a jit-ted function that you \n",
    "        can call\n",
    "    \"\"\"\n",
    "    # Simulation\n",
    "\n",
    "    spike_train = []\n",
    "    for i, t in enumerate(jnp.arange(0, t_max, dt)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        V, spiked= run_step(V, v_thresh, v_reset, W, tau_m, dt, membr_R)\n",
    "        spike_train.append(spiked)\n",
    "    return spike_train\n",
    "\n",
    "time_arr = []\n",
    "for i in range(num_simulations):\n",
    "    start = time.time()\n",
    "    spike_train = run_simulation(\n",
    "        W,\n",
    "        _V,\n",
    "        _tau_m, _V_reset, _V_thresh, _R,\n",
    "        t_max=_t_max, dt=_dt\n",
    "    )\n",
    "    np.asarray(spike_train)\n",
    "    end = time.time()\n",
    "    print(f\"Iteration {i} took: {end - start} seconds\")\n",
    "    time_arr.append(end - start)\n",
    "    if i == 1:\n",
    "        print(\"Breaking out - point proven\")\n",
    "        break\n",
    "\n",
    "print(f\"Average Time: {np.mean(time_arr)}\")\n",
    "print(f\"S.Dev Time: {np.std(time_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da305ad7-8d91-4223-a90d-c1353ab4e9e3",
   "metadata": {},
   "source": [
    "# What gives? \n",
    "\n",
    "`jax.jit` doesn't always play nicely with numpy! There are times where calling `jnp.asarray` is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506fc2de-aa78-41ca-81eb-3124c115c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 took: 5.72046685218811 seconds\n",
      "Iteration 1 took: 5.695834159851074 seconds\n",
      "Iteration 2 took: 5.726809740066528 seconds\n",
      "Average Time: 5.714370250701904\n",
      "S.Dev Time: 0.01336034068134031\n"
     ]
    }
   ],
   "source": [
    "time_arr = []\n",
    "for i in range(num_simulations):\n",
    "    start = time.time()\n",
    "    spike_train = run_simulation(\n",
    "        jnp.asarray(W),\n",
    "        _V,\n",
    "        _tau_m, _V_reset, _V_thresh, _R,\n",
    "        _t_max, _dt\n",
    "    )\n",
    "    np.asarray(spike_train)\n",
    "    end = time.time()\n",
    "    print(f\"Iteration {i} took: {end - start} seconds\")\n",
    "    time_arr.append(end - start)\n",
    "\n",
    "print(f\"Average Time: {np.mean(time_arr)}\")\n",
    "print(f\"S.Dev Time: {np.std(time_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91d6be-440a-4dba-88f3-3dac786a27da",
   "metadata": {},
   "source": [
    "# Further Exercises:\n",
    "\n",
    "1) Read through [extras_when_not_to_jit.ipynb](./extras_when_not_to_jit.ipynb)\n",
    "\n",
    "2) Read through [Jax AoT Compiling](https://jax.readthedocs.io/en/latest/aot.html) and take note of the limitations that come with Jax's AoT compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17ec77-7bb1-4a33-8bfd-f8271e2ad573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
