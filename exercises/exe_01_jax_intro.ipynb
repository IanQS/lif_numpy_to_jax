{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8623bd1c-abab-4eaf-bd15-4f78d335de0b",
   "metadata": {},
   "source": [
    "# Intro to Jax a.k.a swapping `np.X` for `jnp.X`\n",
    "\n",
    "## Lesson Goals:\n",
    "\n",
    "By the end of this lesson, you will have an understanding of how to migrate from `numpy` to `jax`, and get a feel for how similar the two libraries can be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ba69c5-8ea0-4d65-a69d-f888d06d2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import TypeAlias\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f80b94-03ee-46a6-86b5-5e773b4f1095",
   "metadata": {},
   "source": [
    "# What is Jax?\n",
    "\n",
    "To put it simply, Jax is numpy for various hardware accelerators. However, it offers much more than that by providing higher-level abstractions, utilizing a different backend (XLA), and supporting automatic differentiation.\n",
    "\n",
    "From the website:\n",
    "\n",
    "> JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
    "\n",
    "Despite these capabilities, not all concepts and idioms from NumPy translate directly, and there are certain ‼️sharp edges‼️ of which you should be aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256009c1-261b-4f65-a78c-177f9997e390",
   "metadata": {},
   "source": [
    "## Sample Exercises\n",
    "\n",
    "Below, we provide some exercises to help you become familiar with Jax and Numpy. The solutions are more or less what you might expect from a drop-in replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3e2125-7b08-4425-b542-59481cd9df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product passed\n",
      "is_even_filter passed\n",
      "top_n_of_norm_squared passed\n",
      "hadamard passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dot_product():\n",
    "    v = np.random.rand(10)\n",
    "    M = np.random.rand(10, 5)\n",
    "\n",
    "    expected_result = np.dot(v, M)\n",
    "    actual_result = jnp.dot(v, M)\n",
    "\n",
    "    assert jnp.allclose(expected_result, actual_result)\n",
    "    print(\"Dot product passed\")\n",
    "    \n",
    "\n",
    "def is_even_filter():\n",
    "    to_filter_np = np.asarray([1, 2, 3, 5, 10, 20])\n",
    "    expected_result = to_filter_np[to_filter_np % 2 == 0]\n",
    "\n",
    "    to_filter_jnp = jnp.asarray(to_filter_np)\n",
    "    \n",
    "    actual_result = to_filter_jnp[to_filter_jnp % 2 == 0]\n",
    "\n",
    "    assert jnp.allclose(expected_result, actual_result)\n",
    "    print(\"is_even_filter passed\")\n",
    "\n",
    "def top_n_of_norm_squared():\n",
    "    M = np.random.rand(10, 5)\n",
    "    TOP_N = 5\n",
    "    \n",
    "    expected_result = np.sort(np.linalg.norm(M @ M.T, axis=1))[::-1][:TOP_N]\n",
    "\n",
    "    jnp_M = jnp.asarray(M)\n",
    "    actual_result = jnp.sort(jnp.linalg.norm(jnp_M @ jnp_M.T, axis=1))[::-1][:TOP_N]\n",
    "\n",
    "    assert jnp.allclose(expected_result, actual_result)\n",
    "    print(\"top_n_of_norm_squared passed\")\n",
    "\n",
    "\n",
    "def hadamard():\n",
    "    M = np.random.rand(10, 5)\n",
    "    expected_result = M * M\n",
    "\n",
    "    jnp_M = jnp.asarray(M)\n",
    "    actual_result = jnp_M * jnp_M # Your code here\n",
    "\n",
    "    assert jnp.allclose(expected_result, actual_result)\n",
    "    print(\"hadamard passed\")\n",
    "    \n",
    "    \n",
    "\n",
    "dot_product()\n",
    "is_even_filter()\n",
    "top_n_of_norm_squared()\n",
    "hadamard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb6f70-a4ed-4c60-a885-afcd3fcf42a6",
   "metadata": {},
   "source": [
    "# Returning to the LIF Example\n",
    "\n",
    "<img src=\"../assets/lif_formulation.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "We now return to the LIF model we discussed earlier. Simply put, we want to replace the np.X calls with jnp.X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbad8c04-4574-47d1-a10f-1d4fe97b1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import (\n",
    "    _dt,\n",
    "    _t_max,\n",
    "    _tau_m,\n",
    "    _V_reset,\n",
    "    _V_thresh,\n",
    "    _R,\n",
    "    num_simulations\n",
    ")\n",
    "\n",
    "\n",
    "with open('weights.npy', 'rb') as f:\n",
    "    W = np.load(f)\n",
    "\n",
    "# Initial conditions\n",
    "n_neurons = len(W)# Number of neurons in the network\n",
    "_V = jnp.ones(n_neurons) * _V_reset  # Initial potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f14d01-6637-4daf-850f-7db1e38b7e56",
   "metadata": {},
   "source": [
    "# Type Definitions for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7932809-ea42-4e5d-9b59-838cd0ac2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor3D: TypeAlias = jnp.ndarray\n",
    "Mat: TypeAlias = jnp.ndarray\n",
    "Vec: TypeAlias = jnp.ndarray "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2191f-f7ae-4c72-b901-65e5344e2784",
   "metadata": {},
   "source": [
    "# Run the Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b4fcc6-6cab-4f63-9672-9b4ea02d13de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_simulations):\n\u001b[1;32m     40\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 41\u001b[0m     spike_train \u001b[38;5;241m=\u001b[39m run_simulation(\n\u001b[1;32m     42\u001b[0m         W,\n\u001b[1;32m     43\u001b[0m         _V,\n\u001b[1;32m     44\u001b[0m         _tau_m, _V_reset, _V_thresh, _R,\n\u001b[1;32m     45\u001b[0m         _t_max, _dt\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m#print(f\"Iteration {i} took: {end - start} seconds\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mrun_simulation\u001b[0;34m(W, V, tau_m, v_reset, v_thresh, membr_R, t_max, dt)\u001b[0m\n\u001b[1;32m     27\u001b[0m spike_train\u001b[38;5;241m.\u001b[39mappend(fired)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Update voltages\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m I_syn \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mdot(spike_train[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Synaptic current from spikes\u001b[39;00m\n\u001b[1;32m     31\u001b[0m dV \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m/\u001b[39m tau_m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39mV \u001b[38;5;241m+\u001b[39m v_reset \u001b[38;5;241m+\u001b[39m membr_R \u001b[38;5;241m*\u001b[39m I_syn)\n\u001b[1;32m     32\u001b[0m V \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dV\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_simulation(\n",
    "    W: Mat,\n",
    "    V: Vec,\n",
    "\n",
    "    # Neuron Parameters\n",
    "    tau_m: float,\n",
    "    v_reset: float,\n",
    "    v_thresh: float,\n",
    "    membr_R: float,\n",
    "\n",
    "    # How long do we run for? \n",
    "    t_max: float,\n",
    "    dt: float, \n",
    "\n",
    "):\n",
    "    # Simulation\n",
    "\n",
    "    spike_train = []\n",
    "    for i, t in enumerate(jnp.arange(0, t_max, dt)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "    \n",
    "        fired = V >= v_thresh\n",
    "        V = jnp.where(fired, v_reset, V)\n",
    "        \n",
    "        # Record spike times\n",
    "        spike_train.append(fired)\n",
    "    \n",
    "        # Update voltages\n",
    "        I_syn = W.dot(spike_train[-1])  # Synaptic current from spikes\n",
    "        dV = (dt / tau_m) * (-V + v_reset + membr_R * I_syn)\n",
    "        V += dV\n",
    "    \n",
    "        # No self-inputs; neurons cannot spike themselves in this timestep\n",
    "        V = jnp.where(fired, v_reset, V)\n",
    "    return spike_train\n",
    "\n",
    "time_arr = []\n",
    "for i in range(num_simulations):\n",
    "    start = time.time()\n",
    "    spike_train = run_simulation(\n",
    "        W,\n",
    "        _V,\n",
    "        _tau_m, _V_reset, _V_thresh, _R,\n",
    "        _t_max, _dt\n",
    "    )\n",
    "    end = time.time()\n",
    "    #print(f\"Iteration {i} took: {end - start} seconds\")\n",
    "    time_arr.append(end - start)\n",
    "\n",
    "print(f\"Average Time: {np.mean(time_arr)}\")\n",
    "print(f\"S.Dev Time: {np.std(time_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83581c3a-aaee-4a73-82bf-5744426283fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
